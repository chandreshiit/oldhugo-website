---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Online sparse class imbalance learning on big data
subtitle: ''
summary: ''
authors:
- Chandresh Kumar Maurya
- Durga Toshniwal
- Gopalan Vijendran Venkoparao
tags:
- Class imbalance learning
- Online learning
- Proximal algorithm
- Big data
categories: []
date: '2016-01-01'
lastmod: 2022-05-05T18:43:51+05:30
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-05-05T13:13:50.902113Z'
publication_types:
- '2'
abstract: 'Class imbalance learning is the study of problems in which some classes
  appear more frequently than the others. Most existing works that study this problem
  assume data set to be dense and do not exploit the rich structure of the data. One
  such structure is the sparsity. In the present work, we focus on solving the class
  imbalance problem under the sparsity assumption. More specifically, a well-known
  Gmean metric for class imbalance learning problem in binary classification setting
  has been maximized, which results in a non-convex loss function. Convex relaxation
  techniques are used to convert the non-convex problem to the convex problem. The
  problem formulation in the present work uses L1 regularized proximal learning framework
  and is solved via accelerated-stochastic-proximal gradient descent algorithm. Our
  aim in the paper is to show: (i) the application of proximal algorithms to solve
  real world problems (class imbalance); (ii) how it scales to Big data; and (iii)
  how it outperforms some recently proposed algorithms in terms of Gmean, F-measure
  and Mistake rate on several benchmark data sets.'
publication: '*Neurocomputing*'
doi: https://doi.org/10.1016/j.neucom.2016.07.040
links:
- name: URL
url_pdf: https://www.sciencedirect.com/science/article/pii/S0925231216307962
---
